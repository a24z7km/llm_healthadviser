# ===================================================================
# â–  ã‚»ãƒ«2ï¼šãƒŸãƒ‹ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚»ãƒ« (ãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼ãƒ»APIã‚­ãƒ¼ä¿®æ­£ç‰ˆ)
# ===================================================================
#
# Colabã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸPDFã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã‹ã‚‰
# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¾ã§ã‚’ä¸€è²«ã—ã¦è¡Œã„ã¾ã™ã€‚
#
# -------------------------------------------------------------------

import os
import gc
import torch
import json
from datasets import load_dataset
from google.colab import userdata
from huggingface_hub import login
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline
)
from peft import LoraConfig
from trl import SFTTrainer
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- 1. åˆæœŸè¨­å®šã¨Hugging Faceã¸ã®ãƒ­ã‚°ã‚¤ãƒ³ ---
print("ğŸ”‘ Hugging Faceã¸ã®ãƒ­ã‚°ã‚¤ãƒ³ã‚’é–‹å§‹ã—ã¾ã™...")

try:
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆå 'TEST2' ã‚’ä½¿ç”¨
    hf_token = userdata.get('TEST2')
    if not hf_token:
        raise ValueError("Hugging Faceã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    login(token=hf_token)
    print("âœ… Hugging Faceã¸ã®ãƒ­ã‚°ã‚¤ãƒ³ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
except Exception as e:
    print(f"âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    print("Colabã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã« 'TEST2' ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    raise e

# --- 2. ãƒ¢ãƒ‡ãƒ«ã¨ãƒªãƒã‚¸ãƒˆãƒªã®è¨­å®š ---
# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
# â˜… å¿…ãšã‚ãªãŸã®Hugging Faceãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ãƒ¢ãƒ‡ãƒ«åã«å¤‰æ›´ã—ã¦ãã ã•ã„ â˜…
# â˜… (ä¾‹: "your-hf-username/My-Sleep-Advisor-v1")          â˜…
# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
NEW_MODEL_REPO_ID = "a24z7km/Llama3-Health-Advisor-v5" # â† ã“ã“ã‚’æ›¸ãæ›ãˆã‚‹

FINETUNE_TARGET_MODEL_ID = "elyza/Llama-3-ELYZA-JP-8B"
DATA_GENERATION_MODEL_ID = "elyza/Llama-3-ELYZA-JP-8B"
OUTPUT_DIR = "./results"


# --- 3. ãƒ­ãƒ¼ã‚«ãƒ«PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè‡ªå‹•ç”Ÿæˆ ---
print("\nãƒ­ãƒ¼ã‚«ãƒ«PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™...")

# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
# â˜… å¿…ãšColabã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã”è‡ªèº«ã®PDFãƒ•ã‚¡ã‚¤ãƒ«åã«å¤‰æ›´ã—ã¦ãã ã•ã„ â˜…
# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
PDF_FILE_PATH = "å¥åº·æ—¥æœ¬21(ç¬¬ä¸‰æ¬¡)æ¨é€²ã®ãŸã‚ã®èª¬æ˜è³‡æ–™â‘ .pdf" # â† ã“ã“ã‚’æ›¸ãæ›ãˆã‚‹

if not os.path.exists(PDF_FILE_PATH):
    raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: '{PDF_FILE_PATH}'ã€‚Colabã«PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«åãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

print(f"ğŸ“š '{PDF_FILE_PATH}' ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
loader = PyPDFLoader(PDF_FILE_PATH)
documents = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
split_docs = text_splitter.split_documents(documents)
print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆã‚’ {len(split_docs)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸã€‚")

# 3-2. ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆç”¨LLMã®ãƒ­ãƒ¼ãƒ‰
print(f"\nğŸ¤– ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆç”¨ã®LLM ({DATA_GENERATION_MODEL_ID}) ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™...")
quantization_config_gen = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)

# æ­£ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³å¤‰æ•° 'hf_token' ã‚’ä½¿ç”¨
generator_tokenizer = AutoTokenizer.from_pretrained(DATA_GENERATION_MODEL_ID, token=hf_token)
generator_model = AutoModelForCausalLM.from_pretrained(
    DATA_GENERATION_MODEL_ID,
    quantization_config=quantization_config_gen,
    device_map="auto",
    token=hf_token
)

pipe = pipeline("text-generation", model=generator_model, tokenizer=generator_tokenizer, torch_dtype=torch.bfloat16)

# 3-3. ãƒ­ãƒ¼ã‚«ãƒ«LLMã«ã‚ˆã‚‹Q&Aãƒšã‚¢ã®ç”Ÿæˆ
print("\nğŸ¤– ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’ä½¿ã£ã¦Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¾ã™...ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰")
qa_generation_prompt_template = """
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
ã‚ãªãŸã¯ã€æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã®ã¿ã«åŸºã¥ã„ã¦ã€è³ªã®é«˜ã„Q&Aãƒšã‚¢ã‚’1ã¤ã ã‘ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ä¸€èˆ¬çš„ãªçŸ¥è­˜ã‚„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ãªã„æƒ…å ±ã¯çµ¶å¯¾ã«è¿½åŠ ã›ãšã€JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
JSONå½¢å¼:
{ "input": "ç”Ÿæˆã—ãŸè³ªå•", "output": "ç”Ÿæˆã—ãŸå›ç­”" }<|eot_id|>
<|start_header_id|>user<|end_header_id|>
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:
{context}
ä¸Šè¨˜ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’ä½¿ã„ã€å®Ÿè·µçš„ã§å…·ä½“çš„ãªå¥åº·ã«é–¢ã™ã‚‹Q&Aãƒšã‚¢ã‚’JSONå½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
"""
qa_pairs = []
for i, doc in enumerate(split_docs):
    print(f"  - ãƒãƒ£ãƒ³ã‚¯ {i+1}/{len(split_docs)} ã‚’å‡¦ç†ä¸­...")
    prompt = qa_generation_prompt_template.format(context=doc.page_content)
    try:
        terminators = [pipe.tokenizer.eos_token_id, pipe.tokenizer.convert_tokens_to_ids("<|eot_id|>")]
        outputs = pipe(prompt, max_new_tokens=512, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9, pad_token_id=pipe.tokenizer.eos_token_id)
        result_text = outputs[0]['generated_text'][len(prompt):].strip()
        json_part = result_text[result_text.find('{'):result_text.rfind('}')+1]
        json.loads(json_part)
        qa_pairs.append(json_part)
    except Exception as e:
        print(f"    âš ï¸ ãƒãƒ£ãƒ³ã‚¯ {i+1} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯ç„¡åŠ¹ãªJSONãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ: {e}")
        continue

# 3-4. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
DATASET_FILE = "generated_sleep_dataset.jsonl"
print(f"\nğŸ’¾ ç”Ÿæˆã—ãŸQ&Aãƒšã‚¢ã‚’ '{DATASET_FILE}' ã«ä¿å­˜ã—ã¦ã„ã¾ã™...")
with open(DATASET_FILE, "w", encoding="utf-8") as f:
    for pair_json_str in qa_pairs:
        f.write(pair_json_str + "\n")
print("âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è‡ªå‹•ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


# 3-5. ãƒ¡ãƒ¢ãƒªè§£æ”¾
print("\nğŸ§¹ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆç”¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã—ã¾ã™...")
del pipe, generator_model, generator_tokenizer
gc.collect()
torch.cuda.empty_cache()
print("âœ… ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã—ã¾ã—ãŸã€‚")

# --- 4. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æº–å‚™ ---
print(f"\nâœ¨ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ãƒ¢ãƒ‡ãƒ« ({FINETUNE_TARGET_MODEL_ID}) ã®æº–å‚™ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

# 4-1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã¨æ•´å½¢
print("ğŸ¥£ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã—ã¦ã„ã¾ã™...")
def format_dataset_phi3(example):
    if not example.get("input") or not example.get("output"): return None
    return {"text": f"<|user|>\n{example['input']}<|end|>\n<|assistant|>\n{example['output']}<|end|>"}

dataset = load_dataset("json", data_files=DATASET_FILE, split="train")
formatted_dataset = dataset.filter(lambda example: example.get("input") and example.get("output"))
formatted_dataset = formatted_dataset.map(format_dataset_phi3)
print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã¨æ•´å½¢ãŒå®Œäº†ã—ã¾ã—ãŸã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {len(formatted_dataset)}")

# 4-2. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™
print("ğŸ“¦ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™...")
quantization_config_ft = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16)
model = AutoModelForCausalLM.from_pretrained(FINETUNE_TARGET_MODEL_ID, quantization_config=quantization_config_ft, device_map="auto", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(FINETUNE_TARGET_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
print("âœ… ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

# --- 5. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ ---
print("\nğŸ”¥ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
peft_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, bias="none", task_type="CAUSAL_LM", target_modules=['qkv_proj', 'o_proj', 'gate_up_proj', 'down_proj'])
training_args = TrainingArguments(output_dir=OUTPUT_DIR, per_device_train_batch_size=1, gradient_accumulation_steps=8, learning_rate=2e-4, num_train_epochs=3, logging_steps=10, report_to="none", save_strategy="no")
trainer = SFTTrainer(model=model, tokenizer=tokenizer, args=training_args, train_dataset=formatted_dataset, peft_config=peft_config, dataset_text_field="text", max_seq_length=2048)
trainer.train()
print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

# --- 6. ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ---
print(f"\nğŸš€ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªãƒã‚¸ãƒˆãƒª '{NEW_MODEL_REPO_ID}' ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™...")
try:
    # æ­£ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³å¤‰æ•° 'hf_token' ã‚’ä½¿ç”¨
    trainer.model.push_to_hub(NEW_MODEL_REPO_ID, private=True, token=hf_token)
    trainer.tokenizer.push_to_hub(NEW_MODEL_REPO_ID, private=True, token=hf_token)
    print(f"âœ…âœ…âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ https://huggingface.co/{NEW_MODEL_REPO_ID} ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
except Exception as e:
    print(f"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# --- 7. æ¨è«–ãƒ†ã‚¹ãƒˆ ---
print("\nğŸ§ª ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™...")
prompt = "æœ€è¿‘ã€ä»•äº‹ã®ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã§å¤œä¸­ã«ç›®ãŒè¦šã‚ã¦ã—ã¾ã„ã¾ã™ã€‚ã©ã†ã—ãŸã‚‰è‰¯ã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ"
chat = [{"role": "user", "content": prompt}]
input_ids = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors="pt")
output_ids = model.generate(input_ids.to(model.device), max_new_tokens=300, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, do_sample=True, temperature=0.7, top_p=0.9)
response = tokenizer.decode(output_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)

print("\nã€è³ªå•ã€‘")
print(prompt)
print("\nã€ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã€‘")
print(response)

print("\nğŸ‰ğŸ‰ğŸ‰ ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Œäº†ã—ã¾ã—ãŸï¼ ğŸ‰ğŸ‰ğŸ‰")
